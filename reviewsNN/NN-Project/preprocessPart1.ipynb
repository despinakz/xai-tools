{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DespoinaK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DespoinaK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\DespoinaK\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DespoinaK\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DespoinaK\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DespoinaK\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DespoinaK\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DespoinaK\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the shape of dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMDB Dataset Sentiment Analysis with NN\n",
    "\n",
    "# -------------------- 1) IMPORT REQUIRED LIBRARIES --------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import xlwt\n",
    "\n",
    "# -------------------- 2) IMPORT AND ANALYZE THE DATASET --------------------\n",
    "\n",
    "#read the csv file using the read_csv function from pd library\n",
    "movie_reviews = pd.read_csv(\"C:\\\\Users\\\\DespoinaK\\\\Desktop\\\\NN-Project\\\\IMDB Dataset.csv\")\n",
    "\n",
    "#check if there are null values or not in csv file\n",
    "movie_reviews.isnull().values.any()\n",
    "\n",
    "#print the shape of our dataset\n",
    "print(\"Here is the shape of dataset.\")\n",
    "movie_reviews.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take the first 8 rows of our dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first 8 rows from dataset\n",
    "print(\"Take the first 8 rows of our dataset.\")\n",
    "movie_reviews.head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the 8th review of the dataset.\n",
      " We see that our text contains punctuations, brackets, and a few HTML tags as well.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take and show the 8th review of our dataset \n",
    "# - We can see that our text contains punctuations, brackets, and a few HTML tags as well. We will preprocess this text in the section (3) below.\n",
    "print(\"Here is the 8th review of the dataset.\\n We see that our text contains punctuations, brackets, and a few HTML tags as well.\")\n",
    "movie_reviews[\"review\"][7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of positive and negative sentiments in dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x255c8e7ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distribution of positive and negative sentiments in our dataset - we see that 25K-Positive and 25K-Negative\n",
    "import seaborn as sns\n",
    "print(\"Distribution of positive and negative sentiments in dataset.\")\n",
    "sns.countplot(x='sentiment', data=movie_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's now again see the 8th review\n",
      "Before Preprocessing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- 3) DATA PREPROCESSING --------------------\n",
    "\n",
    "\n",
    "# We saw that our dataset contained punctuations and HTML tags. \n",
    "# In this section we will define a function that takes a text string as a parameter \n",
    "# and then performs preprocessing on the string to remove special characters and HTML tags from the string. \n",
    "# Finally, the string is returned to the calling function.\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # -------------------- 3.1) REMOVE HTML TAGS --------------------\n",
    "    #remove_tags function: simply replaces anything between opening and closing <> with an empty space\n",
    "    clean = re.compile('<.*?>')\n",
    "    sentence = re.sub(clean, ' ', sen)\n",
    "\n",
    "    # -------------------- 3.2) REMOVE PUNCTUATIONS AND NUMBERS --------------------\n",
    "    # Especially everything is removed except capital and small English letters, which results in single characters that make no sense. \n",
    "    # For instance, when you remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. \n",
    "    # Hence, we are left with single character \"s\".'''\n",
    "    \n",
    "    word1 = \"can't\" #it's a special exception because when we cut the apostrophe, then the can't --> can that we don't want it\n",
    "    word2 = \"won't\" #it's a special exception because when we cut the apostrophe, then the won't --> won that we don't want it\n",
    "    \n",
    "    if word1 in sentence:\n",
    "        sentence = sentence.replace(word1,\"can not\")\n",
    "    \n",
    "    if word2 in sen:\n",
    "        sentence = sentence.replace(word2,\"will not\")\n",
    "    \n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence) #remove punctuations and numbers\n",
    "    \n",
    "    \n",
    "    # -------------------- 3.3) SINGLE CHARACTER REMOVAL --------------------\n",
    "    #remove all the single characters and replace it by a space which creates multiple spaces in our text\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # -------------------- 3.4) REMOVE MULTIPLE SPACES --------------------\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # -------------------- 3.5) CONVERT TO LOWERCASE --------------------\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    negations = {    'aren':'are not', \n",
    "                     'hasn':'has not',\n",
    "                     'wasn':'was not',\n",
    "                  '  doesn':'does not',\n",
    "                  'shouldn':'should not',\n",
    "                     'didn':'did not',\n",
    "                    'mustn':'must not',\n",
    "                     'hadn':'had not',\n",
    "                    'weren':'were not',\n",
    "                     'shan':'shall not',\n",
    "                    'needn':'need not',\n",
    "                   'wouldn':'would not',\n",
    "                      'don':'do not',\n",
    "                      'ain':'is not',\n",
    "                    'haven':'have not',\n",
    "                      'isn':'is not',\n",
    "                   'mightn':'might not',\n",
    "                   'couldn':'could not' }\n",
    "    \n",
    "    #replace only the whole word and NOT the part of the word (e.g aren --> are not BUT arena --> arena)\n",
    "    sentence = ' '.join(negations[i] if i in negations else i for i in sentence.split())\n",
    "    \n",
    "    # -------------------- 3.5) STOP-WORDS REMOVAL --------------------\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    stop_words = set(stopwords.words('english')) - set(['no','nor','any','few','not'])\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    sentence = TreebankWordDetokenizer().detokenize(filtered_sentence)\n",
    "    \n",
    "    # -------------------- 3.6) STEMMING or LEMMATIZATION or BOTH???? --------------------\n",
    "    #Let's trying the lemmatization approach as it is more accurate \n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "\n",
    "    sentence = word_tokenize(sentence)\n",
    "    #print(sen)\n",
    "    filteredSen = [] \n",
    "    for i in sentence:\n",
    "        wordnetPOS = get_wordnet_pos(i)\n",
    "        #print (wordnetPOS)\n",
    "\n",
    "        # 2. Lemmatize Single Word with the appropriate POS tag\n",
    "        lemma = lemmatizer.lemmatize(i,wordnetPOS)\n",
    "        #print(\"lemma of \"+i+\" is: \"+lemma)\n",
    "        i = i.replace(i,lemma)\n",
    "        filteredSen.append(i)\n",
    " \n",
    "        sentence = TreebankWordDetokenizer().detokenize(filteredSen)      \n",
    "        #print(sen)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    tag_dict = {   'NN':'n',\n",
    "                  'NNS':'n',\n",
    "                  'NNP':'n',\n",
    "                 'NNPS':'n',\n",
    "                   'JJ':'a',\n",
    "                  'JJR':'a',\n",
    "                  'JJS':'a',\n",
    "                   'RB':'r',\n",
    "                  'RBR':'r',\n",
    "                  'RBS':'r',\n",
    "                  'VB':'v',\n",
    "                  'VBD':'v',\n",
    "                  'VBG':'v',\n",
    "                  'VBN':'v',\n",
    "                  'VBP':'v',\n",
    "                  'VBZ':'v'}\n",
    "    \n",
    "    \n",
    "    wordPOS = nltk.pos_tag([word])\n",
    "    #print (wordPOS[0][0],wordPOS[0][1])\n",
    "    tag = wordPOS[0][1]\n",
    "    \n",
    "    if tag in tag_dict:  \n",
    "        return tag_dict.get(tag)\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "    \n",
    "    \n",
    "# Now we will preprocess our reviews and will store them in a new list\n",
    "X = [] #creates X array (empty)\n",
    "sentences = list(movie_reviews['review']) #save all reviews in a list named 'sentences'\n",
    "\n",
    "#for all reviews in the list apply the function 'preprocess_text()'\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen)) #save the result of preprocessing for each review to the X array \n",
    "    \n",
    "#WRITE XSL File\n",
    "wb = xlwt.Workbook()\n",
    "\n",
    "sheet1 = wb.add_sheet('Sheet 1')\n",
    "#sheet.write(0, 0,'review,sentiment')\n",
    "\n",
    "for i in range(50000):\n",
    "    sheet1.write(i,1,X[i])\n",
    "    sheet1.write(i,2,movie_reviews['sentiment'][i])\n",
    "    \n",
    "wb.save('C:\\\\Users\\\\DespoinaK\\\\Desktop\\\\preprocessedData.xls')   \n",
    "\n",
    "#Example: Let's now again see the fourth review\n",
    "print(\"Let's now again see the 8th review\")\n",
    "print(\"Before Preprocessing\")\n",
    "sentences[7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After Preprocessing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'show amaze fresh innovative idea first air first year brilliant thing drop show not really funny anymore continued decline complete waste time today truly disgraceful far show fall write painfully bad performance almost bad not mildly entertain respite guest host show probably would not still air find hard believe creator hand select original cast also chose band hack follow one recognize brilliance see fit replace mediocrity felt must give star respect original cast make show huge success show awful not believe still air'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"After Preprocessing\")\n",
    "X[7]\n",
    "# printing the array X (all preprocessing reviews) \n",
    "#for i in range(len(X)): \n",
    "    #print (X[i]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
